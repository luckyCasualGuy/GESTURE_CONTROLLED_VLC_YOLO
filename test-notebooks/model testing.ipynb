{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from cv2 import VideoCapture, cvtColor, COLOR_BGR2RGB, resize\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Path Check:\n",
      "----------\n",
      "MODEL_PATH <- ../model/yolov3-tiny-416-int8.tflite\n",
      "status: True\n",
      "----------\n",
      "VIDEO_PATH <- ../sample/showcase.mp4\n",
      "status: True\n",
      "----------\n",
      "SAMPLE_PATH <- ../sample/sample.jpg\n",
      "status: True\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = '../model/yolov3-tiny-416-int8.tflite'\n",
    "VIDEO_PATH = '../sample/showcase.mp4'\n",
    "SAMPLE_PATH = '../sample/sample.jpg'\n",
    "\n",
    "print(f'''\n",
    "Path Check:\n",
    "{'-'*10}\n",
    "MODEL_PATH <- {MODEL_PATH}\n",
    "status: {Path(MODEL_PATH).exists()}\n",
    "{'-'*10}\n",
    "VIDEO_PATH <- {VIDEO_PATH}\n",
    "status: {Path(VIDEO_PATH).exists()}\n",
    "{'-'*10}\n",
    "SAMPLE_PATH <- {SAMPLE_PATH}\n",
    "status: {Path(SAMPLE_PATH).exists()}\n",
    "{'-'*10}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=MODEL_PATH)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Details\n",
      "----------\n",
      "Input Details:\n",
      "[{'name': 'input_1', 'index': 0, 'shape': array([  1, 416, 416,   3]), 'shape_signature': array([ -1, 416, 416,   3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "----------\n",
      "Output Details:\n",
      "[{'name': 'Identity', 'index': 167, 'shape': array([   1, 2535,    2]), 'shape_signature': array([ 1, -1,  2]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_1', 'index': 188, 'shape': array([   1, 2535,    4]), 'shape_signature': array([ 1, -1,  4]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(f'''\n",
    "Model Details\n",
    "{'-'*10}\n",
    "Input Details:\n",
    "{input_details}\n",
    "{'-'*10}\n",
    "Output Details:\n",
    "{output_details}\n",
    "{'-'*10}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_shape <- [  1 416 416   3]\n",
      "input_size <- (416, 416)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_shape = input_details[0]['shape']\n",
    "input_size = tuple(input_shape[1:3])\n",
    "\n",
    "print(f'''\n",
    "input_shape <- {input_shape}\n",
    "input_size <- {input_size}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera Not Working !! <br>Temperorily using Video Feed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "reading loop works:\n",
      "----------\n",
      "Sample Video Frames: 608\n",
      "----------\n",
      "Frame that was captured:\n",
      "SAMPLE_FRAME <- (492, 656, 3)\n",
      "----------\n",
      "WE NOT USING THIS FRAME AS HARD TO FIND FRAME WITH PREDICTION!!\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cap = VideoCapture(VIDEO_PATH)\n",
    "\n",
    "if not cap.isOpened(): \n",
    "    print(\"Error opening video stream or file\")\n",
    "    \n",
    "\n",
    "#Only for using as Example:\n",
    "# ------------------------\n",
    "SAMPLE_FRAME = None\n",
    "count = 0\n",
    "# ------------------------\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    #Taking 1 frame for Example\n",
    "    # ------------------------\n",
    "    if count == 225: SAMPLE_FRAME = frame\n",
    "    count += 1\n",
    "    # ------------------------\n",
    "    \n",
    "    if not ret: break\n",
    "\n",
    "print(f'''\n",
    "reading loop works:\n",
    "{'-'*10}\n",
    "Sample Video Frames: {count}\n",
    "{'-'*10}\n",
    "Frame that was captured:\n",
    "SAMPLE_FRAME <- {SAMPLE_FRAME.shape}\n",
    "{'-'*10}\n",
    "WE NOT USING THIS FRAME AS HARD TO FIND FRAME WITH PREDICTION!!\n",
    "{'-'*10}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions Here ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "Frame that will be used as sample\n",
      "ANOTHER_SAMPLE_FRAME <- (667, 889, 3)\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ANOTHER_SAMPLE_FRAME = np.asarray(Image.open(SAMPLE_PATH))\n",
    "\n",
    "print(f'''\n",
    "{'-'*10}\n",
    "Frame that will be used as sample\n",
    "ANOTHER_SAMPLE_FRAME <- {ANOTHER_SAMPLE_FRAME.shape}\n",
    "{'-'*10}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_frame_image \n",
      "----------\n",
      "\n",
      "Frame shold resized\n",
      "Before: (667, 889, 3)\n",
      "After: (416, 416, 3)\n",
      "\n",
      "Normalizing\n",
      "Before: [ 7  6 12]\n",
      "After: [0.02745098 0.02352941 0.04705882]\n",
      "\n",
      "Expanding Dimensions\n",
      "Before: (416, 416, 3)\n",
      "After: (1, 416, 416, 3)\n",
      "\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_frame_image(frame, log=False):\n",
    "#     _, frame = cap.read() hm...\n",
    "    if log: print(f\"get_frame_image \\n{'-'*10}\\n\")\n",
    "    \n",
    "    if log: print(f'converted to COLOR_BGR2RGB\\nBefore: {frame[0][0][:11]}')\n",
    "    frame = cvtColor(frame, COLOR_BGR2RGB)\n",
    "    if log: print(f'After: {frame[0][0][:11]}\\n')\n",
    "    \n",
    "    if log: print(f'Frame shold resized\\nBefore: {frame.shape}')\n",
    "    image_data = resize(frame, input_size)\n",
    "    before_norm = image_data\n",
    "    if log: print(f'After: {image_data.shape}\\n')\n",
    "        \n",
    "    if log: print(f'Normalizing\\nBefore: {image_data[0][0][:11]}')\n",
    "    image_data = image_data / 255.\n",
    "    if log: print(f'After: {image_data[0][0][:11]}\\n')\n",
    "    \n",
    "    if log: print(f'Expanding Dimensions\\nBefore: {image_data.shape}')\n",
    "    image_data = image_data[np.newaxis, ...].astype(np.float32)\n",
    "    if log: print(f'After: {image_data.shape}\\n')\n",
    "        \n",
    "    if log: print(f\"{'-'*10}\\n\")\n",
    "        \n",
    "    return frame, image_data, before_norm\n",
    "\n",
    "ORIGINAL_IMAGE, IMAGE_TO_PRIDICT, BEFORE_NORM = get_frame_image(ANOTHER_SAMPLE_FRAME, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.fromarray(BEFORE_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_pass\n",
      "----------\n",
      "Predictions: [array([[[1.9111993e-05, 1.5654377e-05],\n",
      "        [1.1842297e-05, 9.2808159e-06],\n",
      "        [4.8030220e-06, 2.7827407e-06],\n",
      "        ...,\n",
      "        [1.5267733e-07, 2.9837591e-07],\n",
      "        [8.9499039e-08, 1.3190538e-07],\n",
      "        [7.7039101e-07, 3.1981273e-07]]], dtype=float32), array([[[  9.258622 ,   6.6577353,   8.323171 ,  65.703735 ],\n",
      "        [ 21.849075 ,   6.213477 ,  17.905348 ,  36.328445 ],\n",
      "        [ 38.847076 ,   6.2583847,  19.240072 ,  39.04131  ],\n",
      "        ...,\n",
      "        [323.95117  , 392.8551   , 380.1087   , 473.58838  ],\n",
      "        [357.4076   , 393.01978  , 378.88672  , 475.02396  ],\n",
      "        [394.93228  , 393.88318  , 343.7354   , 500.18134  ]]],\n",
      "      dtype=float32)]\n",
      "Prediction Shape: [(1, 2535, 2), (1, 2535, 4)]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "def forward_pass(image, log=False):\n",
    "    if log: print(f\"forward_pass\\n{'-'*10}\")\n",
    "        \n",
    "    interpreter.set_tensor(input_details[0]['index'], image)\n",
    "    interpreter.invoke()\n",
    "    pred = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n",
    "    \n",
    "    if log: print(f\"Predictions: {pred}\\nPrediction Shape: {[p.shape for p in pred]}\\n{'-'*10}\")\n",
    "    return pred\n",
    "\n",
    "prediction = forward_pass(IMAGE_TO_PRIDICT, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions:\n",
    "**`[(1, 2535, 2), (1, 2535, 4)]`**  \n",
    "**`scores`**: (1, 2535, 2)  \n",
    "**`Box Prediction`**: (1, 2535, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "SAMPLE_PREDICTION <- [array([[[1.9111993e-05, 1.5654377e-05],\n",
      "        [1.1842297e-05, 9.2808159e-06],\n",
      "        [4.8030220e-06, 2.7827407e-06],\n",
      "        ...,\n",
      "        [1.5267733e-07, 2.9837591e-07],\n",
      "        [8.9499039e-08, 1.3190538e-07],\n",
      "        [7.7039101e-07, 3.1981273e-07]]], dtype=float32), array([[[  9.258622 ,   6.6577353,   8.323171 ,  65.703735 ],\n",
      "        [ 21.849075 ,   6.213477 ,  17.905348 ,  36.328445 ],\n",
      "        [ 38.847076 ,   6.2583847,  19.240072 ,  39.04131  ],\n",
      "        ...,\n",
      "        [323.95117  , 392.8551   , 380.1087   , 473.58838  ],\n",
      "        [357.4076   , 393.01978  , 378.88672  , 475.02396  ],\n",
      "        [394.93228  , 393.88318  , 343.7354   , 500.18134  ]]],\n",
      "      dtype=float32)]\n",
      "Size: [(1, 2535, 2), (1, 2535, 4)]\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_PREDICTION = [pa[:, :, :] for pa in prediction]\n",
    "\n",
    "print(f'''\n",
    "{'-'*10}\n",
    "SAMPLE_PREDICTION <- {SAMPLE_PREDICTION}\n",
    "Size: {[a.shape for a in SAMPLE_PREDICTION]}\n",
    "{'-'*10}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter_boxes\n",
      "----------\n",
      "Reducing Max Scores\n",
      "[We have 2535 classes here we select class with highest probability\n",
      "Before: (1, 2535, 2) 1/5: [1.9111993e-05 1.5654377e-05]\n",
      "After: (1, 2535) 1/5: 1.9111992514808662e-05\n",
      "\n",
      "Creating Mask for filtering score_max\n",
      "Keeping probabilities above treshhold: 0.25\n",
      "score_max: [[1.9111993e-05 1.1842297e-05 4.8030220e-06 ... 2.9837591e-07\n",
      "  1.3190538e-07 7.7039101e-07]]\n",
      "Mask: [[False False False ... False False False]] -> (1, 2535)\n",
      "\n",
      "Filtering Boxes wrt score_max Mask\n",
      "box_xywh Before: (1, 2535, 4)\n",
      "box_xywh After: (5, 4)\n",
      "\n",
      "Filtering scores wrt score_max Mask\n",
      "score Before: (1, 2535, 2)\n",
      "score After: (5, 2)\n",
      "\n",
      "Reshaping class boxes\n",
      "class_boxes Before: (5, 4)\n",
      "After: (1, 5, 4)\n",
      "\n",
      "Reshaping pred confidence\n",
      "pred_conf Before: (5, 2)\n",
      "pred_conf After: (1, 5, 2)\n",
      "\n",
      "Splitting class boxes\n",
      "class_boxes Before: (1, 5, 4)\n",
      "After [xy, wh]: [TensorShape([1, 5, 2]), TensorShape([1, 5, 2])]\n",
      "\n",
      "Changing input shape type\n",
      "Before: (tf.int32, TensorShape([1, 2]))\n",
      "input shape After: <dtype: 'float32'>\n",
      "\n",
      "Changing xy to yx\n",
      "box xy Before: [[[145.21246 170.47766]\n",
      "  [344.2243  178.11557]\n",
      "  [345.45734 199.91832]\n",
      "  [149.06737 169.7414 ]\n",
      "  [341.84036 176.51276]]]\n",
      "box yx After: [[[170.47766 145.21246]\n",
      "  [178.11557 344.2243 ]\n",
      "  [199.91832 345.45734]\n",
      "  [169.7414  149.06737]\n",
      "  [176.51276 341.84036]]]\n",
      "\n",
      "Changing hw to wh\n",
      "box xy Before: [[[190.45915 227.71674]\n",
      "  [156.27277 276.2465 ]\n",
      "  [178.54697 213.12839]\n",
      "  [220.21927 228.49756]\n",
      "  [170.49619 350.5121 ]]]\n",
      "box yx After: [[[227.71674 190.45915]\n",
      "  [276.2465  156.27277]\n",
      "  [213.12839 178.54697]\n",
      "  [228.49756 220.21927]\n",
      "  [350.5121  170.49619]]]\n",
      "\n",
      "Getting the corners of rectangle to draw it\n",
      "box bottom corners: [[[0.13610408 0.12015117]\n",
      "  [0.09613539 0.63963443]\n",
      "  [0.22440895 0.61582655]\n",
      "  [0.1333957  0.0936484 ]\n",
      "  [0.00302095 0.61680835]]]\n",
      "box upper corners: [[[0.68350005 0.5779857 ]\n",
      "  [0.7601895  1.0152901 ]\n",
      "  [0.73673683 1.0450261 ]\n",
      "  [0.6826687  0.62302166]\n",
      "  [0.8455981  1.026655  ]]]\n",
      "\n",
      "Finally concating upper and lower corners\n",
      "Final Boxes: [[[0.13610408 0.12015117 0.68350005 0.5779857 ]\n",
      "  [0.09613539 0.63963443 0.7601895  1.0152901 ]\n",
      "  [0.22440895 0.61582655 0.73673683 1.0450261 ]\n",
      "  [0.1333957  0.0936484  0.6826687  0.62302166]\n",
      "  [0.00302095 0.61680835 0.8455981  1.026655  ]]]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "def filter_boxes(box_xywh, scores, score_threshold=0.4, input_shape = tf.constant([416,416]), log=False):\n",
    "    if log: print(f\"filter_boxes\\n{'-'*10}\")\n",
    "        \n",
    "    if log: print(f\"Reducing Max Scores\\n[We have {scores.shape[1]} classes here we select class with highest probability\\nBefore: {scores.shape} 1/5: {scores[0, 0]}\")\n",
    "    scores_max = tf.math.reduce_max(scores, axis=-1)\n",
    "    if log: print(f\"After: {scores_max.shape} 1/5: {scores_max[0, 0]}\\n\")\n",
    "    \n",
    "    if log: print(f\"Creating Mask for filtering score_max\\nKeeping probabilities above treshhold: {score_threshold}\\nscore_max: {scores_max}\")\n",
    "    mask = scores_max >= score_threshold\n",
    "    if log: print(f\"Mask: {mask} -> {mask.shape}\\n\")\n",
    "                  \n",
    "    if log: print(f\"Filtering Boxes wrt score_max Mask\\nbox_xywh Before: {box_xywh.shape}\")     \n",
    "    class_boxes = tf.boolean_mask(box_xywh, mask)\n",
    "    if log: print(f\"box_xywh After: {class_boxes.shape}\\n\")\n",
    "    \n",
    "    if log: print(f\"Filtering scores wrt score_max Mask\\nscore Before: {scores.shape}\")\n",
    "    pred_conf = tf.boolean_mask(scores, mask)\n",
    "    if log: print(f\"score After: {pred_conf.shape}\\n\")\n",
    "    \n",
    "    if log: print(f\"Reshaping class boxes\\nclass_boxes Before: {class_boxes.shape}\")\n",
    "    class_boxes = tf.reshape(class_boxes, [tf.shape(scores)[0], -1, tf.shape(class_boxes)[-1]])\n",
    "    if log: print(f\"After: {class_boxes.shape}\\n\")\n",
    "    \n",
    "    if log: print(f\"Reshaping pred confidence\\npred_conf Before: {pred_conf.shape}\")\n",
    "    pred_conf = tf.reshape(pred_conf, [tf.shape(scores)[0], -1, tf.shape(pred_conf)[-1]])\n",
    "    if log: print(f\"pred_conf After: {pred_conf.shape}\\n\")\n",
    "    \n",
    "    if log: print(f\"Splitting class boxes\\nclass_boxes Before: {class_boxes.shape}\")\n",
    "    box_xy, box_wh = tf.split(class_boxes, (2, 2), axis=-1)\n",
    "    if log: print(f\"After [xy, wh]: {[box_xy.shape, box_wh.shape]}\\n\")\n",
    "        \n",
    "    if log: print(f\"Changing input shape type\\nBefore: {input_shape.dtype, input_shape.shape}\")\n",
    "    input_shape = tf.cast(input_shape, dtype=tf.float32)\n",
    "    if log: print(f\"input shape After: {input_shape.dtype}\\n\")\n",
    "    \n",
    "    if log: print(f\"Changing xy to yx\\nbox xy Before: {box_xy}\")\n",
    "    box_yx = box_xy[..., ::-1]\n",
    "    if log: print(f\"box yx After: {box_yx}\\n\")\n",
    "    \n",
    "    if log: print(f\"Changing hw to wh\\nbox xy Before: {box_wh}\")\n",
    "    box_hw = box_wh[..., ::-1]\n",
    "    if log: print(f\"box yx After: {box_hw}\\n\")\n",
    "    \n",
    "    if log: print(f\"Getting the corners of rectangle to draw it\")\n",
    "    box_mins = (box_yx - (box_hw / 2.))/ input_shape\n",
    "    box_maxes = (box_yx + (box_hw / 2.))/ input_shape\n",
    "    if log: print(f\"box bottom corners: {box_mins}\\nbox upper corners: {box_maxes}\\n\")\n",
    "    \n",
    "    if log: print(f\"Finally concating upper and lower corners\")\n",
    "    boxes = tf.concat([\n",
    "        box_mins[..., 0:1],  # y_min\n",
    "        box_mins[..., 1:2],  # x_min\n",
    "        box_maxes[..., 0:1],  # y_max\n",
    "        box_maxes[..., 1:2]  # x_max\n",
    "    ], axis=-1)\n",
    "    if log: print(f\"Final Boxes: {boxes}\\n{'-'*10}\")\n",
    "    \n",
    "    return (boxes, pred_conf)\n",
    "\n",
    "boxes, pred_conf = filter_boxes(SAMPLE_PREDICTION[1], SAMPLE_PREDICTION[0], \n",
    "                                score_threshold=0.25, input_shape=tf.constant([input_size]), log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Non Max Supression Parameters\n",
      "----------\n",
      "Boxes Before: (1, 5, 4)\n",
      "Boxes After: (1, 5, 1, 4)\n",
      "\n",
      "Pred confidense i.e scores Before: (1, 5, 2)\n",
      "scores After: (1, 5, 2)\n",
      "\n",
      "IOU threshhold set to: 40.0%\n",
      "SCORE threshold set to: 25.0%\n",
      "\n",
      "----------\n",
      "Results\n",
      "classes <- [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]] shape: (1, 50)\n",
      "valid detection <- [2] shape: (1,)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# NON MAX SUPRESSION -> Used to remove box overlappings.\n",
    "IOU = 0.4 # -> tolerence of overlapping boxes measured as IOU\n",
    "SCORE = 0.25\n",
    "\n",
    "print(f\"Performing Non Max Supression Parameters\\n{'-'*10}\")\n",
    "\n",
    "print(f\"Boxes Before: {boxes.shape}\")\n",
    "nm_s_boxes = tf.reshape(boxes, (tf.shape(boxes)[0], -1, 1, 4))\n",
    "print(f\"Boxes After: {nm_s_boxes.shape}\\n\")\n",
    "\n",
    "print(f\"Pred confidense i.e scores Before: {pred_conf.shape}\")\n",
    "nm_s_scores = tf.reshape(pred_conf, (tf.shape(pred_conf)[0], -1, tf.shape(pred_conf)[-1]))\n",
    "print(f\"scores After: {nm_s_scores.shape}\\n\")\n",
    "\n",
    "print(f'IOU threshhold set to: {IOU*100}%\\nSCORE threshold set to: {SCORE*100}%\\n')\n",
    "\n",
    "classes, valid_detection = tf.image.combined_non_max_suppression(\n",
    "            boxes=tf.reshape(boxes, (tf.shape(boxes)[0], -1, 1, 4)),\n",
    "            scores=tf.reshape(pred_conf, (tf.shape(pred_conf)[0], -1, tf.shape(pred_conf)[-1])),\n",
    "            max_output_size_per_class=50,\n",
    "            max_total_size=50,\n",
    "            iou_threshold=IOU,\n",
    "            score_threshold=SCORE)[2:]\n",
    "\n",
    "print(f\"{'-'*10}\")\n",
    "print(f\"Results\\nclasses <- {classes} shape: {classes.shape}\\nvalid detection <- {valid_detection} shape: {valid_detection.shape}\")\n",
    "print(f\"{'-'*10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classes < lists index of every class that was detected <br>valid detection < lists How many values in `classes` are vaid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Items i.e classes that were finally detected: [0, 1]\n",
      "These are the classes that were detected\n"
     ]
    }
   ],
   "source": [
    "detected_items = [classes.numpy()[0][i].astype(int) for i in range(valid_detection.numpy()[0])]\n",
    "\n",
    "print(f\"Detected Items i.e classes that were finally detected: {detected_items}\\nThese are the classes that were detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain that we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA56D5BE0>\n",
      "Frame 1: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5C9DDF0>\n",
      "Frame 2: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5C9D190>\n",
      "Frame 3: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5C9D400>\n",
      "Frame 4: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5C98AF0>\n",
      "Frame 5: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5C9D190>\n",
      "Frame 6: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5C9D400>\n",
      "Frame 7: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5C9D160>\n",
      "Frame 8: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5C9D400>\n",
      "Frame 9: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5C9D190>\n",
      "Frame 10: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5C9D190>\n",
      "Frame 11: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5C9D190>\n",
      "Frame 12: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5A193D0>\n",
      "Frame 13: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5A19100>\n",
      "Frame 14: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5A19100>\n",
      "Frame 15: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5A19100>\n",
      "Frame 16: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5A19100>\n",
      "Frame 17: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5A168E0>\n",
      "Frame 18: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5A16220>\n",
      "Frame 19: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5A16220>\n",
      "Frame 20: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5A168E0>\n",
      "Frame 21: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5CA8B80>\n",
      "Frame 22: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5CA86D0>\n",
      "Frame 23: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5CA8B80>\n",
      "Frame 24: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5CA8130>\n",
      "Frame 25: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5CA86D0>\n",
      "Frame 26: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5CA8130>\n",
      "Frame 27: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5CA8130>\n",
      "Frame 28: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5CA8B80>\n",
      "Frame 29: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5CA86D0>\n",
      "Frame 30: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5CA86D0>\n",
      "Frame 31: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5CA86D0>\n",
      "Frame 32: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21F93B30CD0>\n",
      "Frame 33: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA5A19EB0>\n",
      "Frame 34: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21F93B30CD0>\n",
      "Frame 35: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA595C490>\n",
      "Frame 36: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA595C490>\n",
      "Frame 37: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA595C490>\n",
      "Frame 38: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21F93B30CD0>\n",
      "Frame 39: [1]\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA595C490>\n",
      "Frame 40: []\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA595C490>\n",
      "Frame 41: []\n",
      "<PIL.Image.Image image mode=RGB size=656x492 at 0x21FA56D5BE0>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-687e6bb3f4f2>\u001b[0m in \u001b[0;36mforward_pass\u001b[1;34m(image, log)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0minterpreter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_details\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0minterpreter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_details\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_details\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\our\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py\u001b[0m in \u001b[0;36minvoke\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    522\u001b[0m     \"\"\"\n\u001b[0;32m    523\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_safe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvoke\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_all_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cap = VideoCapture(VIDEO_PATH)\n",
    "\n",
    "if not cap.isOpened(): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "IOU = 0.4\n",
    "SCORE = 0.25\n",
    "\n",
    "count = 0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    count += 1\n",
    "    \n",
    "    if not ret: break\n",
    "        \n",
    "    \n",
    "    # we dont need ORIGINAL IMAGE\n",
    "    ORIGINAL_IMAGE, IMAGE_TO_PRIDICT, BEFORE_NORM = get_frame_image(frame)\n",
    "    \n",
    "    prediction = forward_pass(IMAGE_TO_PRIDICT)\n",
    "    \n",
    "    boxes, pred_conf = filter_boxes(prediction[1], prediction[0], \n",
    "                                score_threshold=0.25, input_shape=tf.constant([input_size]))\n",
    "    \n",
    "    classes, valid_detection = tf.image.combined_non_max_suppression(\n",
    "            boxes=tf.reshape(boxes, (tf.shape(boxes)[0], -1, 1, 4)),\n",
    "            scores=tf.reshape(pred_conf, (tf.shape(pred_conf)[0], -1, tf.shape(pred_conf)[-1])),\n",
    "            max_output_size_per_class=50,\n",
    "            max_total_size=50,\n",
    "            iou_threshold=IOU,\n",
    "            score_threshold=SCORE)[2:]\n",
    "\n",
    "    detected_items = classes.numpy()[0, :valid_detection.numpy()[0]].astype(int)\n",
    "#     detected_items = [classes.numpy()[0][i].astype(int) for i in range(valid_detection.numpy()[0])]\n",
    "    \n",
    "    print(f\"Frame {count}: {detected_items}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
